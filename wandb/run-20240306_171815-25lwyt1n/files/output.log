[34m[1mwandb[39m[22m: logging graph, to disable use `wandb.watch(log_graph=False)`
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.
Traceback (most recent call last):
  File "/kyukon/home/gent/438/vsc43883/OOD_eval/main.py", line 261, in <module>
    train_step(args.config_file)
  File "/kyukon/home/gent/438/vsc43883/OOD_eval/main.py", line 189, in train_step
    trainer.fit(model,  train_dataloaders =DataLoader.train_dataloader(), val_dataloaders = DataLoader.val_dataloader())
  File "/apps/gent/RHEL8/zen3-ampere-ib/software/PyTorch-Lightning/1.8.4-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/pytorch_lightning/core/hooks.py", line 443, in train_dataloader
    raise MisconfigurationException("`train_dataloader` must be implemented to be used with the Lightning Trainer")
lightning_lite.utilities.exceptions.MisconfigurationException: `train_dataloader` must be implemented to be used with the Lightning Trainer